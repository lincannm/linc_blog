[{"title":"如何在WSL的Ubuntu 24.04上通过VcXsrv使用Xfce桌面","path":"/article/b54c182.html","content":"感觉在VMware上用Ubuntu的图形界面太卡了，所以就用上WSL（介绍：什么是适用于 Linux 的 Windows 子系统 | Microsoft Learn）了。通过xrdp连接Gnome和KDE桌面都挺卡，使用X11协议（通过VcXsrv）则不会有太大卡顿。然而可惜KDE在VcXsrv上会有显示问题，怎么也未能解决，只能用Xfce桌面了。网上现有的资料都是用VcXsrv显示Xfce桌面，也不知道为什么不提提别的桌面，别的桌面（如KDE）有问题也稍微提一嘴啊。。 安装WSL，安装Ubuntu 24.04此处不多说，自行看官方文档。 安装 WSL | Microsoft Learn 网络模式设置为Mirrored打开WSL Settings，来到网络页面，网络模式改成Mirrored（桥接），否则不能跟VcXsrv建立连接。改完别忘了wsl --shutdown。 VcXsrv电脑上可以用Xming或VcXsrv来通过X11协议使用桌面，不过Xming就坑爹了，2025年更新的最新版本却需要捐助才能下载，免费下载的版本竟停留在2007年。07年的版本实测会卡死，还是用免费开源的VcXsrv吧！听说WSL微软推荐使用VcXsrv，但我暂时没有在官方文档看见这样的描述。 安装在这里下载：Releases · marchaesen&#x2F;vcxsrv 安装完后不知为何不会显示在开始菜单上的应用程序列表上，只会在桌面建立一个快捷方式。点击这个快捷方式启动就好。 启动 选择One large window 默认 勾选Disable access control 完成 Xfce1. 安装1sudo apt install -y xfce4 xorg # 安装Xfce和xorg 2. 设置DISPLAY环境变量（关键）1sudo vim .bashrc 在最后一行写上： 1export DISPLAY=localhost:0 使.bashrc生效： 1source ~/.bashrc 这样就行。有资料说要依照/etc/resolf.conf的nameserver的ip进行设置，试了一下无效。我又设置成:0.0，VcXsrv不会显示画面。搞了不知道多久了，自己试了下localhost，竟然ok了。。。 3. 启动Xfce执行： 1sudo startxfce4 回到VcXsrv，xfce就有了！ 可以执行sudo startxfce4 &amp;让他后台执行。 问题如果用KDE桌面并用startplasma-x11启动的话，看起来会变这样（已经进入桌面）： 窗口会被这一坨黑挡住。什么破玩意。。 只能用Xfce了，然而以前在Termux早把Xfce玩惯了。还是回到VMware吧！ （Gnome桌面并没有测试，因为懒）","tags":["Linux","WSL","Ubuntu","Xfce"],"categories":["Linux"]},{"title":"解决Hexo+Typora引用图片不方便问题","path":"/article/f83728a5.html","content":"参考：hexo+typora+github图片路径问题 - 简书 参照 资源文件夹 | Hexo 对_config.yml进行如下设置： 1234post_asset_folder: truemarked: prependRoot: true postAsset: true Typora进行如下设置： 然而在typora粘贴图片后，typora可以渲染，hexo由于路径问题，渲染失败。 在typora中，图片路径为测试文章/001.png；而在hexo的网页上，图片路径被渲染为http://localhost:4000/测试文章/001.png。而只有在typora中把图片路径写作001.png，才能在网页中被正确渲染为http://localhost:4000/2025/02/06/测试文章/001.png。但这样的话，typora不就因为文件路径不存在而无法显示图片了吗？ 查了点资料，只能手动修改hexo-render-marked模块的renderer.js了。在以下位置添加代码： 123if(href.indexOf(&quot;/&quot;)&gt;0)&#123;\thref=href.split(&#x27;/&#x27;)[1];&#125; 问题解决。。。","tags":["Hexo","Typora"],"categories":["Hexo"]},{"title":"在780M核显笔记本上通过LM Studio本地化部署DeepSeek大模型","path":"/article/c73044ca.html","content":"本来想用Ollama部署LLM，但我下载启动Ollama后发现官方不支持我的Radeon 780M核显（gfx1103）： 123456789101112C:\\Users\\lincannm&gt;ollama serve2025/02/01 18:01:29 routes.go:1187: INFO server config env=&quot;map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\\\Users\\\\lincannm\\\\.ollama\\\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]&quot;time=2025-02-01T18:01:29.821+08:00 level=INFO source=images.go:432 msg=&quot;total blobs: 0&quot;time=2025-02-01T18:01:29.822+08:00 level=INFO source=images.go:439 msg=&quot;total unused blobs removed: 0&quot;time=2025-02-01T18:01:29.823+08:00 level=INFO source=routes.go:1238 msg=&quot;Listening on 127.0.0.1:11434 (version 0.5.7)&quot;time=2025-02-01T18:01:29.825+08:00 level=INFO source=routes.go:1267 msg=&quot;Dynamic LLM libraries&quot; runners=&quot;[cuda_v12_avx rocm_avx cpu cpu_avx cpu_avx2 cuda_v11_avx]&quot;time=2025-02-01T18:01:29.825+08:00 level=INFO source=gpu.go:226 msg=&quot;looking for compatible GPUs&quot;time=2025-02-01T18:01:29.825+08:00 level=INFO source=gpu_windows.go:167 msg=packages count=1time=2025-02-01T18:01:29.825+08:00 level=INFO source=gpu_windows.go:214 msg=&quot;&quot; package=0 cores=8 efficiency=0 threads=16time=2025-02-01T18:01:30.397+08:00 level=WARN source=amd_windows.go:140 msg=&quot;amdgpu is not supported (supported types:[gfx1030 gfx1100 gfx1101 gfx1102 gfx906])&quot; gpu_type=gfx1103 gpu=0 library=C:\\Users\\lincannm\\AppData\\Local\\Programs\\Ollama\\lib\\ollamatime=2025-02-01T18:01:30.400+08:00 level=INFO source=gpu.go:392 msg=&quot;no compatible GPUs were discovered&quot;time=2025-02-01T18:01:30.400+08:00 level=INFO source=types.go:131 msg=&quot;inference compute&quot; id=0 library=cpu variant=avx2 compute=&quot;&quot; driver=0.0 name=&quot;&quot; total=&quot;27.8 GiB&quot; available=&quot;20.3 GiB&quot; 听说还有个叫做LM Studio的东西，对我这样的780M核显支持不错，可以直接用。而且Ollama是命令行使用，而LM Studio则是GUI操作，更加人性化。 下载与安装LM Studio官网：https://lmstudio.ai/ 下载它： 安装就不必多说了吧。 使用点开侧边栏第四个图标，弹出此“发现”窗口。首先到这里看看，由于我的GPU是AMD Radeon 780M，在这里看看确保选中了Vulkan选项，这样运行LLM的时候才会调用到显卡。 App Settings可以设置界面为中文。 到“Model Search”下载DeekSeek模型。由于我的显存只有可怜的4G，我就下载大小为4G左右的7B版本就好。 下载完成！回到聊天界面，到顶栏选择模型。 根据电脑配置配置这些配置~（GPU Offload被译为GPU卸载有点别扭，网上查了一下，其实就是根据当前GPU的显存性能来调节线条，值越高，就意味着运行模型的性能占比交给GPU的更高） 点一下加载模型就加载好了。部署到此结束，整个过程都非常简单。 开始聊天，思考过程中笔记本风扇直接起飞。 回复太人机了，完整版DeepSeek会跟我解析这段话的幽默性。这样也很正常，完整版671B，我才7B的模型根本比不了。 网络优化我是翻墙出去下载的模型，如果没有翻墙条件的话可以打开LM Studio安装目录，用vscode把目录下所有文件做全局替换——把所有huggingface.co替换为hf-mirror.com即可。 问题不知道为什么我的核显占用很小，几乎闲着，而且一把GPU Offload调大就报Unknown error。。。全网找不到解决办法。 后来我下载了14B的模型，终于懂得解析幽默性了，这下就聪明多了。但是我让它用贴吧语气锐评ChatGPT，依旧不聪明，甚至都不知道是在骂ChatGPT还是在骂我。。。 问题解决核显占用小的问题解决了！搞了两天原来更新一下amd显卡驱动就好了。。。更新了之后，GPU Offload可以全部拉满，可以全部交给显卡计算了。 Ollama-for-amd在那个问题解决之前，试了一下Ollama-for-amd项目（https://github.com/likelovewant/ollama-for-amd/），该项目针对amd显卡进行支持性扩展，通过这个项目我的780M也能跑ollama了。安装的话直接下载Release里面的exe就行了。 然而聊天会有bug，不仅没有深度思考过程，后面我每说一句它还只会直接接我的话。不过既然问题解决了，干脆接着用LMStudio算了吧。","tags":["LLM"],"categories":["摸鱼"]},{"title":"创建了一个基于Hexo和Vercel的博客！","path":"/article/b8b788b0.html","content":"春节快乐🎉🎉！突发奇想想搞个博客网站，于是就花了15块钱（首年）买下 linc.work 这个域名，通过 Hexo + Redefine主题 + Github + Vercel 搭建了这个网站（另外通过 LeanCloud + Vercel 搞了文章下的 Waline 评论系统）。 搞个博客网站主要是可以分享点东西，记录点东西，当然直接原因还是春节无聊搞着玩的。有群友叫我玩玩Hugo，但是我玩过，不会玩，还是Hexo在网上的资料多一点，适合小白操作。博客这玩意不少搞计算机的都有，我不搞显得我不专业（ 博客折腾了两天才搞好。我在我电脑上用VMware装了个Ubuntu 24.04.1，安装nodejs，然后搭建hexo环境，安装Redefine主题，之后用 hexo deploy 把博客的静态页面部署到GitHub上，再通过Vercel把GitHub仓库部署到Vercel上面，最后再把域名解析到Vercel上边就好了。 几乎三年没玩技术了，直到最近才听说Vercel挺好用，用了一下还真是个好东西。除了部署静态页面，后端也都能部署（比GitHub Pages牛），而且还能免费使用（白嫖），确实牛逼。 （头图pid：115320144）","tags":["第一篇文章"],"categories":["摸鱼"]},{"title":"关于","path":"/about/index.html","content":"自述现读大一，经过高中三年摸鱼，目前啥也不精（悲），只是了解点Web开发（技术栈为 HTML&#x2F;CSS&#x2F;JS&#x2F;PHP，然而没想到现在Web技术花样越来越多，NodeJS、Vue、React、EJS、SASS、Stylus这的那的，tm真跟不上时代了）和Java而已，刚学会学校教的 C 语言。我还会点版式设计，以前寻思要给班里和动漫社搞海报学的，水平不高。 2025 年过年的时候闲着没事干就搞了这个网站。 Lincannm 是我不知道四年级还是五年级的时候设置的网名，源自注册百度账号的时候，本来想定为 Lincan （姓名前两字拼音），发现名字被占用，于是加了个n，发现名字依旧被占用，一气之下再加了个m，注册成功…… 本博客的头像是从我的GitHub加载过来的，是初中时候的QQ头像。QQ早就换头像了（本站浏览器标签上的小缩略图），但GitHub上一直没换，现在也懒得换了。"}]