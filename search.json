[{"title":"对学校的融合门户登录系统与学工打卡系统进行逆向api","path":"/article/dfd1638c.html","content":"学工系统做得真丑，所以我对学校从登录到晚打卡的api调用流程，进行了抓包逆向的研究分析。 我们学校使用了SSO方案进行统一登录认证，也就是在访问学校各个系统之前，需要在一个 统一身份认证平台（即融合门户） 进行统一登录。只要一次登录就能访问各个系统（比如教务系统、学工系统）。要晚打卡，则需要先保证用户在CAS系统已登录的情况下，才能在学工系统登录并打卡。 本文还未编辑完全因为我的账号开始总是不需要短信验证了，所以就不知道（丢了抓包日志）二重验证相关的API的响应完整该长啥样了。以后有机会短信验证了再补上。 开始分析由于学工系统的定位功能似乎调用了微信sdk，所以电脑浏览器是定位不了的，而且不成功定位就不能打卡，也就不能抓包打卡api，也就意味着我必须用实体手机在微信内置浏览器里进行打卡。我试过这样一种方法： 电脑开启Charles抓包工具 手机安装movecerts这个Magisk模块安装Charles的SSL证书 手机LSPosed开启TrustMeAlready和JustTrustMe模块强制令软件信任Charles的证书 坑爹的是，手机其他任何程序都能抓包，只有微信内置浏览器不能抓包。后来发现可能是因为微信内置浏览器使用了SSL Pinning机制，Charles的自签名SSL证书无法绕过这个机制。 因此，我使用了DevTools的inspector工具对微信内置浏览器进行远程调试，在电脑Chrome的DevTools通过网络面板查看api调用情况。 首先，手机微信访问http://debugxweb.qq.com/?inspector=true激活微信的网页调试，然后用usb连接手机，手机打开usb调试，就可以在电脑上面使用chrome访问chrome://inspect/#devices进行远程调试了。 （图为晚打卡API请求体） 然而，在inspector下，某些请求只显示预配标头（后来发现，与真实请求头的区别只是少了Cookie字段）： 按网上说的禁用缓存等方法依旧没啥用。幸运的是，打卡的关键api不会这样。所以，对于登录流程的逆向，还是得用Chrome的DevTools才行。 另外，我也依旧用了Charles进行抓包辅助，还通过Claude Code接入了chrome-devtools-mcp帮忙分析。 登录和打卡的api并不复杂，所以具体过程就不细说了。经过抓包逆向分析，API的流程大概如下。 API总体流程概述登录融合门户，大体流程是：先调用/cas/mfa/detect检查是否需要双因素验证（一般是手机验证码） -&gt; 调用/cas/mfa/initByType/securephone获取gid -&gt; 调用/attest/api/guard/securephone/send发送短信验证码（传入gid） -&gt; 调用/attest/api/guard/securephone/valid验证短信验证码（传入gid） -&gt; 调用/cas/login登录 登录学工系统，大体流程是：访问学工系统 -&gt; 302至CAS（附加查询参数service，值为来源url） -&gt; CAS系统获取Cookie的TOC字段，判断是否登录 -&gt; 再次302到学工系统（目标url追加ticket这个查询参数）-&gt; 学工系统写入JSESSIONID这个cookie字段 -&gt; 成功登录学工系统 进行晚打卡，则是直接POST https://xgyd.mku.edu.cn/acmc-weichat/wxapp/swkjjksb/mrdk_save.do即可。 具体原理登录 - cas.mku.edu.cn登录的具体逻辑，直接位于登录页面body里的script标签中。 POST &#x2F;cas&#x2F;mfa&#x2F;detectContent-Type: application/x-www-form-urlencoded 请求体： 1username=用户名&amp;password=RSA加密后的密码 这里的密码，是通过/cas/jwt/publicKey提供的公钥使用PKCS1_v1_5算法加密的，并使用base64编码，之后在前面加上__RSA__前缀。 响应： 1&#123;&quot;code&quot;:0,&quot;data&quot;:&#123;&quot;mfaTypeSecurePhone&quot;:true,&quot;mfaTypeQrCode&quot;:false,&quot;need&quot;:false,&quot;mfaTypeAppPush&quot;:false,&quot;mfaTypeFaceVerify&quot;:false,&quot;mfaEnabled&quot;:true,&quot;state&quot;:&quot;TtMomI&quot;,&quot;mfaTypeSecureEmail&quot;:true&#125;&#125; 如果”need”:true，则需要手机短信验证码验证。如果”need”:false，就可以直接登录，跳过以下的短信验证部分。 GET &#x2F;cas&#x2F;mfa&#x2F;initByType&#x2F;securephone查询参数： ​\tstate: 在前面/cas/mfa/detect获取到的data.state 响应： 1(TODO) 如果不需要短信验证，响应： 1&#123;&quot;code&quot;:-1,&quot;error&quot;:&#123;&quot;message&quot;:&quot;fail_0&quot;&#125;&#125; POST &#x2F;attest&#x2F;api&#x2F;guard&#x2F;securephone&#x2F;sendContent-Type: application/json 请求体： 123&#123; &quot;gid&quot;: &quot;前面获取到的gid&quot;&#125; 响应： 1(TODO) POST &#x2F;attest&#x2F;api&#x2F;guard&#x2F;securephone&#x2F;valid请求体： 1234&#123;\t&quot;code&quot;:&quot;短信验证码&quot;,\t&quot;gid&quot;:&quot;前面获取到的gid&quot;&#125; 响应： 1(TODO) POST &#x2F;cas&#x2F;login进行正式的登录 Content-Type: application/x-www-form-urlencoded 请求体（虽然是表单格式，但是为了看着清晰，这里改用字典表示）： 12345678910111213&#123;\t&quot;username&quot;: 用户名,\t&quot;password&quot;: RSA加密后密码,\t&quot;captcha&quot;: &quot;&quot;,\t&quot;currentMenu&quot;: &quot;1&quot;,\t&quot;failN&quot;: &quot;-1&quot;,\t&quot;mfaState&quot;: 在前面`/cas/mfa/detect`获取到的`data.state`,\t&quot;execution&quot;: 在`/cas/login`获取到的execution,\t&quot;_eventId&quot;: &quot;submit&quot;,\t&quot;geolocation&quot;: &quot;&quot;,\t&quot;fpVisitorId&quot;: &quot;&quot;（浏览器指纹，可留空）,\t&quot;submit1&quot;: &quot;Login1&quot;&#125; execution的值在/cas/login登录页面（也就是说得先获取一下网页源码）的name为execution的表单字段里： 1&lt;input type=&quot;hidden&quot; name=&quot;execution&quot; value=&quot;c5087400-a375-41ca-9118-100d9d1d284c_ZXlKaGJHY2lPaUpJVXpVeE1pSjkueXpaeG55NWh。。。此处省略一大坨。。。&quot;&gt; 响应为 200 就是登录成功，返回登陆成功的html。 GET &#x2F;cas&#x2F;login参数： ​\tservice：目标服务的url，比如说要访问的学工系统的URL 响应：302到service指定url上，附加ticket查询参数，例如?ticket=ST-237915-rHwNNRG72-W6VlZknzOE1gjtnO5qU0Du 实际上，直接访问学工系统就会302到当前所述的API上，然后带着ticket302到service指定url，最后写入cookie（JSESSIONID），并不带ticket原地重新302 打卡 - xgyd.mku.edu.cnPOST &#x2F;acmc-weichat&#x2F;wxapp&#x2F;swkjjksb&#x2F;mrdk_save.doContent-Type: application/x-www-form-urlencoded 请求体（虽然是表单格式，但是为了看着清晰，这里改用字典表示）： 123456789101112131415&#123; &quot;id&quot;: &quot;&quot;, &quot;xsid&quot;: 获取的xsid, &quot;jd&quot;: 118.47673, &quot;wd&quot;: 25.03694, &quot;dqszd&quot;: 350583, &quot;drsfzxid&quot;: 1, &quot;sbrq&quot;: &quot;2025-12-29&quot;, &quot;dqszdmc&quot;: &quot;福建省泉州市南安市&quot;, &quot;tw&quot;: 36.5, &quot;dqszdxxdz&quot;: &quot;康美校区&quot;, &quot;ycms&quot;: &quot;&quot;, &quot;twid&quot;: 1, &quot;jzkid&quot;: 1&#125; xsid得先获取/acmc-weichat/wxapp/swkjjksb/mrdk_edit打卡页面的网页源码，在name为xsid的表单字段里： 1&lt;input type=&quot;hidden&quot; id=&quot;xsid&quot; value=&quot;F8C8743F99********28D9831DCD81FF&quot; autocomplete=&quot;off&quot;&gt; 响应： ​\t&#123;&quot;ret&quot;:&quot;ok&quot;&#125;：打卡成功 ​\t&#123;&quot;ret&quot;:&quot;more&quot;&#125;：重复打卡 打卡系统后端存在的问题逆向后发现了这些毛病，特别是第一点，也是奇异搞笑。 后端没有对打卡时间做合法性验证，只有前端做了。导致可以做到一整天24h随时打卡，甚至还能给明天、后天、以后打卡 历史遗留疫情时期的体温相关字段，没啥意义 请求字段全是拼音缩写，毫不直观，全靠猜","tags":["Web","Python"],"categories":["Web"]},{"title":"修改SDDM登录界面DPI","path":"/article/f3576d70.html","content":"sddm在高分辨率屏上面界面会显示得很小。根据网上的资料，可以修改sddm配置文件（&#x2F;usr&#x2F;lib&#x2F;sddm.conf.d&#x2F;default.conf）里的ServerArguments值为ServerArguments=-nolisten tcp -dpi 192。然而修改后毫无卵用，研究了好几天，终于发现了正确方法。 根据https://forums.opensuse.org/t/ui-scaling-in-sddm-no-longer-works-after-kde-plasma-6-upgrade/173350/2，得修改GreeterEnvironment才行。 在/etc/sddm.conf.d/下创建个conf文件，名字随意，可以命名为hidpi.conf。内容如下： [General] GreeterEnvironment=QT_SCREEN_SCALE_FACTORS=2,QT_FONT_DPI=192 注销后进入sddm，可以看见生效了。","tags":["Linux","KDE","SDDM"]},{"title":"如何在WSL的Ubuntu 24.04上通过VcXsrv使用Xfce桌面","path":"/article/b54c182.html","content":"感觉在VMware上用Ubuntu的图形界面太卡了，所以就用上WSL（介绍：什么是适用于 Linux 的 Windows 子系统 | Microsoft Learn）了。通过xrdp连接Gnome和KDE桌面都挺卡，使用X11协议（通过VcXsrv）则不会有太大卡顿。然而可惜KDE在VcXsrv上会有显示问题，怎么也未能解决，只能用Xfce桌面了。网上现有的资料都是用VcXsrv显示Xfce桌面，也不知道为什么不提提别的桌面，别的桌面（如KDE）有问题也稍微提一嘴啊。。 安装WSL，安装Ubuntu 24.04此处不多说，自行看官方文档。 安装 WSL | Microsoft Learn 网络模式设置为Mirrored打开WSL Settings，来到网络页面，网络模式改成Mirrored（桥接），否则不能跟VcXsrv建立连接。改完别忘了wsl --shutdown。 VcXsrv电脑上可以用Xming或VcXsrv来通过X11协议使用桌面，不过Xming就坑爹了，2025年更新的最新版本却需要捐助才能下载，免费下载的版本竟停留在2007年。07年的版本实测会卡死，还是用免费开源的VcXsrv吧！听说WSL微软推荐使用VcXsrv，但我暂时没有在官方文档看见这样的描述。 安装在这里下载：Releases · marchaesen&#x2F;vcxsrv 安装完后不知为何不会显示在开始菜单上的应用程序列表上，只会在桌面建立一个快捷方式。点击这个快捷方式启动就好。 启动 选择One large window 默认 勾选Disable access control 完成 Xfce1. 安装1sudo apt install -y xfce4 xorg # 安装Xfce和xorg 2. 设置DISPLAY环境变量（关键）1sudo vim .bashrc 在最后一行写上： 1export DISPLAY=localhost:0 使.bashrc生效： 1source ~/.bashrc 这样就行。有资料说要依照/etc/resolf.conf的nameserver的ip进行设置，试了一下无效。我又设置成:0.0，VcXsrv不会显示画面。搞了不知道多久了，自己试了下localhost，竟然ok了。。。 3. 启动Xfce执行： 1sudo startxfce4 回到VcXsrv，xfce就有了！ 可以执行sudo startxfce4 &amp;让他后台执行。 问题如果用KDE桌面并用startplasma-x11启动的话，看起来会变这样（已经进入桌面）： 窗口会被这一坨黑挡住。什么破玩意。。 只能用Xfce了，然而以前在Termux早把Xfce玩惯了。还是回到VMware吧！ （Gnome桌面并没有测试，因为懒）","tags":["Linux","WSL","Ubuntu","Xfce"],"categories":["Linux"]},{"title":"解决Hexo+Typora引用图片不方便问题","path":"/article/f83728a5.html","content":"参考：hexo+typora+github图片路径问题 - 简书 参照 资源文件夹 | Hexo 对_config.yml进行如下设置： 1234post_asset_folder: truemarked: prependRoot: true postAsset: true Typora进行如下设置： 然而在typora粘贴图片后，typora可以渲染，hexo由于路径问题，渲染失败。 在typora中，图片路径为测试文章/001.png；而在hexo的网页上，图片路径被渲染为http://localhost:4000/测试文章/001.png。而只有在typora中把图片路径写作001.png，才能在网页中被正确渲染为http://localhost:4000/2025/02/06/测试文章/001.png。但这样的话，typora不就因为文件路径不存在而无法显示图片了吗？ 查了点资料，只能手动修改hexo-render-marked模块的renderer.js了。在以下位置添加代码： 123if(href.indexOf(&quot;/&quot;)&gt;0)&#123;\thref=href.split(&#x27;/&#x27;)[1];&#125; 问题解决。。。","tags":["Hexo","Typora"],"categories":["Hexo"]},{"title":"在780M核显笔记本上通过LM Studio本地化部署DeepSeek大模型","path":"/article/c73044ca.html","content":"本来想用Ollama部署LLM，但我下载启动Ollama后发现官方不支持我的Radeon 780M核显（gfx1103）： 123456789101112C:\\Users\\lincannm&gt;ollama serve2025/02/01 18:01:29 routes.go:1187: INFO server config env=&quot;map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\\\Users\\\\lincannm\\\\.ollama\\\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]&quot;time=2025-02-01T18:01:29.821+08:00 level=INFO source=images.go:432 msg=&quot;total blobs: 0&quot;time=2025-02-01T18:01:29.822+08:00 level=INFO source=images.go:439 msg=&quot;total unused blobs removed: 0&quot;time=2025-02-01T18:01:29.823+08:00 level=INFO source=routes.go:1238 msg=&quot;Listening on 127.0.0.1:11434 (version 0.5.7)&quot;time=2025-02-01T18:01:29.825+08:00 level=INFO source=routes.go:1267 msg=&quot;Dynamic LLM libraries&quot; runners=&quot;[cuda_v12_avx rocm_avx cpu cpu_avx cpu_avx2 cuda_v11_avx]&quot;time=2025-02-01T18:01:29.825+08:00 level=INFO source=gpu.go:226 msg=&quot;looking for compatible GPUs&quot;time=2025-02-01T18:01:29.825+08:00 level=INFO source=gpu_windows.go:167 msg=packages count=1time=2025-02-01T18:01:29.825+08:00 level=INFO source=gpu_windows.go:214 msg=&quot;&quot; package=0 cores=8 efficiency=0 threads=16time=2025-02-01T18:01:30.397+08:00 level=WARN source=amd_windows.go:140 msg=&quot;amdgpu is not supported (supported types:[gfx1030 gfx1100 gfx1101 gfx1102 gfx906])&quot; gpu_type=gfx1103 gpu=0 library=C:\\Users\\lincannm\\AppData\\Local\\Programs\\Ollama\\lib\\ollamatime=2025-02-01T18:01:30.400+08:00 level=INFO source=gpu.go:392 msg=&quot;no compatible GPUs were discovered&quot;time=2025-02-01T18:01:30.400+08:00 level=INFO source=types.go:131 msg=&quot;inference compute&quot; id=0 library=cpu variant=avx2 compute=&quot;&quot; driver=0.0 name=&quot;&quot; total=&quot;27.8 GiB&quot; available=&quot;20.3 GiB&quot; 听说还有个叫做LM Studio的东西，对我这样的780M核显支持不错，可以直接用。而且Ollama是命令行使用，而LM Studio则是GUI操作，更加人性化。 下载与安装LM Studio官网：https://lmstudio.ai/ 下载它： 安装就不必多说了吧。 使用点开侧边栏第四个图标，弹出此“发现”窗口。首先到这里看看，由于我的GPU是AMD Radeon 780M，在这里看看确保选中了Vulkan选项，这样运行LLM的时候才会调用到显卡。 App Settings可以设置界面为中文。 到“Model Search”下载DeekSeek模型。由于我的显存只有可怜的4G，我就下载大小为4G左右的7B版本就好。 下载完成！回到聊天界面，到顶栏选择模型。 根据电脑配置配置这些配置~（GPU Offload被译为GPU卸载有点别扭，网上查了一下，其实就是根据当前GPU的显存性能来调节线条，值越高，就意味着运行模型的性能占比交给GPU的更高） 点一下加载模型就加载好了。部署到此结束，整个过程都非常简单。 开始聊天，思考过程中笔记本风扇直接起飞。 回复太人机了，完整版DeepSeek会跟我解析这段话的幽默性。这样也很正常，完整版671B，我才7B的模型根本比不了。 网络优化我是翻墙出去下载的模型，如果没有翻墙条件的话可以打开LM Studio安装目录，用vscode把目录下所有文件做全局替换——把所有huggingface.co替换为hf-mirror.com即可。 问题不知道为什么我的核显占用很小，几乎闲着，而且一把GPU Offload调大就报Unknown error。。。全网找不到解决办法。 后来我下载了14B的模型，终于懂得解析幽默性了，这下就聪明多了。但是我让它用贴吧语气锐评ChatGPT，依旧不聪明，甚至都不知道是在骂ChatGPT还是在骂我。。。 问题解决核显占用小的问题解决了！搞了两天原来更新一下amd显卡驱动就好了。。。更新了之后，GPU Offload可以全部拉满，可以全部交给显卡计算了。 Ollama-for-amd在那个问题解决之前，试了一下Ollama-for-amd项目（https://github.com/likelovewant/ollama-for-amd/），该项目针对amd显卡进行支持性扩展，通过这个项目我的780M也能跑ollama了。安装的话直接下载Release里面的exe就行了。 然而聊天会有bug，不仅没有深度思考过程，后面我每说一句它还只会直接接我的话。不过既然问题解决了，干脆接着用LMStudio算了吧。","tags":["LLM"],"categories":["摸鱼"]},{"title":"创建了一个基于Hexo和Vercel的博客！","path":"/article/b8b788b0.html","content":"春节快乐🎉🎉！突发奇想想搞个博客网站，于是就花了15块钱（首年）买下 linc.work 这个域名，通过 Hexo + Redefine主题 + Github + Vercel 搭建了这个网站（另外通过 LeanCloud + Vercel 搞了文章下的 Waline 评论系统）。 搞个博客网站主要是可以分享点东西，记录点东西，当然直接原因还是春节无聊搞着玩的。有群友叫我玩玩Hugo，但是我玩过，不会玩，还是Hexo在网上的资料多一点，适合小白操作。博客这玩意不少搞计算机的都有，我不搞显得我不专业（ 博客折腾了两天才搞好。我在我电脑上用VMware装了个Ubuntu 24.04.1，安装nodejs，然后搭建hexo环境，安装Redefine主题，之后用 hexo deploy 把博客的静态页面部署到GitHub上，再通过Vercel把GitHub仓库部署到Vercel上面，最后再把域名解析到Vercel上边就好了。 几乎三年没玩技术了，直到最近才听说Vercel挺好用，用了一下还真是个好东西。除了部署静态页面，后端也都能部署（比GitHub Pages牛），而且还能免费使用（白嫖），确实牛逼。 （头图pid：115320144）","tags":["第一篇文章"],"categories":["摸鱼"]},{"title":"关于","path":"/about/index.html","content":"自述现读大一，经过高中三年摸鱼，目前啥也不精（悲），只是了解点Web开发（技术栈为 HTML&#x2F;CSS&#x2F;JS&#x2F;PHP，然而没想到现在Web技术花样越来越多，NodeJS、Vue、React、EJS、SASS、Stylus这的那的，tm真跟不上时代了）和Java而已，刚学会学校教的 C 语言。我还会点版式设计，以前寻思要给班里和动漫社搞海报学的，水平不高。 2025 年过年的时候闲着没事干就搞了这个网站。 Lincannm 是我不知道四年级还是五年级的时候设置的网名，源自注册百度账号的时候，本来想定为 Lincan （姓名前两字拼音），发现名字被占用，于是加了个n，发现名字依旧被占用，一气之下再加了个m，注册成功…… 本博客的头像是从我的GitHub加载过来的，是初中时候的QQ头像。QQ早就换头像了（本站浏览器标签上的小缩略图），但GitHub上一直没换，现在也懒得换了。"}]